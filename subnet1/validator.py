import logging
import random
import base64
import time
import datetime
from typing import Any, Dict, List, Optional
from collections import defaultdict
import os
import binascii
import uuid
import sys
import asyncio

# Import tá»« SDK Moderntensor (Ä‘Ã£ cÃ i Ä‘áº·t)
try:
    from moderntensor.mt_aptos.consensus.node import ValidatorNode
    from moderntensor.mt_aptos.core.datatypes import TaskAssignment, MinerResult, ValidatorScore, ValidatorInfo, MinerInfo
    # Import successful, use real classes
    USING_MOCK_CLASSES = False
    logging.info("âœ… Successfully imported ValidatorNode and core datatypes from SDK")
except ImportError as e:
    logging.error(f"Could not import ValidatorNode or core datatypes from the SDK: {e}. "
                  "Ensure the 'moderntensor' SDK is installed.")
    # Lá»›p giáº£ Ä‘á»ƒ trÃ¡nh lá»—i náº¿u import tháº¥t báº¡i
    class ValidatorNode:
        def __init__(self, *args, **kwargs):
             self.validator_scores = {}
             self.results_received = defaultdict(list)
             self.tasks_sent = {}
             self.info = type('obj', (object,), {'uid': 'fake_validator_uid'})()
        def _create_task_data(self, miner_uid: str) -> Any: return None
        # XÃ³a score_miner_results giáº£ láº­p
    class TaskAssignment: 
        def __init__(self, task_id, miner_uid, task_data):
            self.task_id = task_id
            self.miner_uid = miner_uid
            self.task_data = task_data
    class MinerResult: 
        def __init__(self, task_id, miner_uid, result_data):
            self.task_id = task_id
            self.miner_uid = miner_uid
            self.result_data = result_data
    class ValidatorScore: pass
    class ValidatorInfo: 
        def __init__(self, uid, **kwargs):
            self.uid = uid
    class MinerInfo: 
        def __init__(self, uid, **kwargs):
            self.uid = uid
    USING_MOCK_CLASSES = True

# Import tá»« cÃ¡c module trong subnet nÃ y
try:
    from .scoring.clip_scorer import calculate_clip_score
except ImportError:
    logging.error("Could not import scoring functions from .scoring.clip_scorer.")
    def calculate_clip_score(*args, **kwargs) -> float: return 0.0

logger = logging.getLogger(__name__)

DEFAULT_PROMPTS = [
    "A photorealistic image of an astronaut riding a horse on the moon.",
    "A watercolor painting of a cozy bookstore cafe in autumn.",
    "A synthwave style cityscape at sunset.",
    "A macro shot of a bee collecting pollen from a sunflower.",
    "A fantasy landscape with floating islands and waterfalls.",
    "A cute dog wearing sunglasses and a party hat.",
    "Impressionist painting of a Parisian street scene.",
    "A steaming bowl of ramen noodles with detailed ingredients.",
    "Cyberpunk warrior standing in a neon-lit alley.",
    "A tranquil zen garden with raked sand and stones.",
]

class Subnet1Validator(ValidatorNode):
    """
    Validator cho Subnet 1 (Image Generation).
    Káº¿ thá»«a ValidatorNode vÃ  triá»ƒn khai logic táº¡o task, cháº¥m Ä‘iá»ƒm áº£nh.
    """

    def __init__(self, *args, **kwargs):
        """Khá»Ÿi táº¡o ValidatorNode vÃ  cÃ¡c thuá»™c tÃ­nh riÃªng cá»§a Subnet 1."""
        # Extract api_port if provided
        self.api_port = kwargs.pop('api_port', None)
        
        super().__init__(*args, **kwargs)
        
        # Set reference to self in core for subnet-specific scoring access
        self.core.validator_instance = self
        
        logger.info(f"âœ¨ [bold]Subnet1Validator[/] initialized for UID: [cyan]{self.info.uid[:10]}...[/]")
        # ThÃªm cÃ¡c khá»Ÿi táº¡o khÃ¡c náº¿u cáº§n, vÃ­ dá»¥:
        # self.image_generation_model = self._load_model()
        # self.clip_scorer = self._load_clip_scorer()

    # --- 1. Override phÆ°Æ¡ng thá»©c táº¡o Task Data ---
    def _create_task_data(self, miner_uid: str) -> Any:
        """
        Táº¡o dá»¯ liá»‡u task (prompt) Ä‘á»ƒ gá»­i cho miner.
        *** ÄÃ£ cáº­p nháº­t Ä‘á»ƒ thÃªm validator_endpoint ***

        Args:
            miner_uid (str): UID cá»§a miner sáº½ nháº­n task (cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ tÃ¹y biáº¿n task).

        Returns:
            Any: Dá»¯ liá»‡u task, trong trÆ°á»ng há»£p nÃ y lÃ  dict chá»©a prompt vÃ  validator_endpoint.
                 Cáº¥u trÃºc nÃ y cáº§n Ä‘Æ°á»£c miner hiá»ƒu.
        """
        selected_prompt = random.choice(DEFAULT_PROMPTS)
        logger.debug(f"Creating task for miner {miner_uid} with prompt: '{selected_prompt}'")

        # Láº¥y API endpoint cá»§a chÃ­nh validator nÃ y tá»« self.info
        # Cáº§n Ä‘áº£m báº£o self.info vÃ  self.info.api_endpoint Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o Ä‘Ãºng
        origin_validator_endpoint = getattr(self.info, 'api_endpoint', None)
        if not origin_validator_endpoint:
             # Xá»­ lÃ½ trÆ°á»ng há»£p endpoint khÃ´ng cÃ³ sáºµn (quan trá»ng)
             logger.error(f"Validator {getattr(self.info, 'uid', 'UNKNOWN')} has no api_endpoint configured in self.info. Cannot create task properly.")
             # CÃ³ thá»ƒ tráº£ vá» None hoáº·c raise lá»—i Ä‘á»ƒ ngÄƒn gá»­i task khÃ´ng Ä‘Ãºng
             return None # Hoáº·c raise ValueError("Validator endpoint missing")

        # Táº¡o deadline vÃ­ dá»¥ (vÃ­ dá»¥: 5 phÃºt ká»ƒ tá»« bÃ¢y giá»)
        now = datetime.datetime.now(datetime.timezone.utc)
        deadline_dt = now + datetime.timedelta(minutes=5)
        deadline_str = deadline_dt.isoformat()

        # Äáº·t priority máº·c Ä‘á»‹nh
        priority_level = random.randint(1, 5)

        # Tráº£ vá» dictionary chá»©a cÃ¡c trÆ°á»ng cáº§n thiáº¿t CHO MINER HIá»‚U
        # Miner sáº½ cáº§n Ä‘á»c 'description' Ä‘á»ƒ láº¥y prompt
        # Miner sáº½ cáº§n Ä‘á»c 'validator_endpoint' Ä‘á»ƒ biáº¿t gá»­i káº¿t quáº£ vá» Ä‘Ã¢u
        return {
            "description": selected_prompt, # Prompt chÃ­nh lÃ  description cá»§a task
            "deadline": deadline_str,
            "priority": priority_level,
            "validator_endpoint": origin_validator_endpoint # <<<--- THÃŠM DÃ’NG NÃ€Y
        }

    # --- Restore the correct override method for scoring ---
    def _score_individual_result(self, task_data: Any, result_data: Any) -> float:
        """
        (Override) Cháº¥m Ä‘iá»ƒm cho má»™t káº¿t quáº£ cá»¥ thá»ƒ tá»« miner cho Subnet 1.
        This method is called by the base ValidatorNode class during its scoring phase.

        Args:
            task_data: Dá»¯ liá»‡u cá»§a task Ä‘Ã£ gá»­i (dict chá»©a 'description' lÃ  prompt).
            result_data: Dá»¯ liá»‡u káº¿t quáº£ miner tráº£ vá» (dict chá»©a 'output_description', etc.).

        Returns:
            Äiá»ƒm sá»‘ float tá»« 0.0 Ä‘áº¿n 1.0.
        """
        logger.debug(f"ğŸ’¯ Scoring result via _score_individual_result...")
        score = 0.0 # Default score
        start_score_time = time.time()
        try:
            # 1. Extract prompt and base64 image
            if not isinstance(task_data, dict) or "description" not in task_data:
                 logger.warning(f"Scoring failed: Task data is not a dict or missing 'description'. Task data: {str(task_data)[:100]}...")
                 return 0.0
            original_prompt = task_data["description"]

            if not isinstance(result_data, dict):
                logger.warning(f"Scoring failed: Received result_data is not a dictionary. Data: {str(result_data)[:100]}...")
                return 0.0
            image_base64 = result_data.get("output_description")
            reported_error = result_data.get("error_details")
            processing_time_ms = result_data.get("processing_time_ms", 0) # Optional

            # 2. Check for errors or missing image
            if reported_error:
                logger.warning(f"Miner reported an error: '{reported_error}'. Assigning score 0.")
                return 0.0
            if not image_base64 or not isinstance(image_base64, str):
                logger.warning(f"No valid image data (base64 string) found in result_data. Assigning score 0. Data: {str(result_data)[:100]}...")
                return 0.0

            # 3. Decode image and Save it
            try:
                image_bytes = base64.b64decode(image_base64)

                # --- Start: Save Image Logic ---
                output_dir = "result_image"
                try:
                    os.makedirs(output_dir, exist_ok=True)
                    # Using placeholder name for now, ideally pass task_id here.
                    miner_uid = result_data.get("miner_uid", "unknown_miner")
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
                    # Need task_id for a truly unique name. Placeholder:
                    filename = f"{output_dir}/result_{miner_uid[:8]}_{timestamp}.png"
                    with open(filename, "wb") as f:
                        f.write(image_bytes)
                    logger.info(f"   Saved image result to: {filename}")
                except OSError as file_err:
                    logger.error(f"   Error saving image file to {filename}: {file_err}")
                except Exception as e:
                    logger.exception(f"   Unexpected error saving image: {e}")
                # --- End: Save Image Logic ---

            except (binascii.Error, ValueError, TypeError) as decode_err:
                 logger.error(f"Scoring failed: Invalid base64 data received. Error: {decode_err}. Assigning score 0.")
                 return 0.0 # Return 0 if decode fails

            # 4. Calculate the actual score using CLIP
            try:
                score = calculate_clip_score(prompt=original_prompt, image_bytes=image_bytes)
                score = max(0.0, min(1.0, score)) # Ensure score is [0, 1]
                logger.info(f"   Calculated score using CLIP: [bold yellow]{score:.4f}[/] (Processing: {processing_time_ms}ms)")
            except ImportError:
                 logger.error("calculate_clip_score function is not available. Assigning score 0.")
                 score = 0.0
            except Exception as clip_err:
                logger.exception(f"Error during CLIP score calculation: {clip_err}. Assigning score 0.")
                score = 0.0

            # Return the calculated score
            scoring_duration = time.time() - start_score_time
            logger.debug(f"ğŸ Finished scoring process in {scoring_duration:.4f}s. Final score: [bold yellow]{score:.4f}[/]")
            return score

        except Exception as e:
            logger.exception(f"ğŸ’¥ Unexpected error during scoring preparation: {e}. Assigning score 0.")
            return 0.0

    # --- KHÃ”NG CÃ’N PHÆ¯Æ NG THá»¨C score_miner_results á» ÄÃ‚Y ---

    # CÃ¡c phÆ°Æ¡ng thá»©c khÃ¡c cá»§a ValidatorNode Ä‘Æ°á»£c káº¿ thá»«a vÃ  sá»­ dá»¥ng logic má»›i.

    def _should_process_result(self, result: MinerResult) -> bool:
        """Kiá»ƒm tra xem káº¿t quáº£ tá»« miner cÃ³ há»£p lá»‡ Ä‘á»ƒ xá»­ lÃ½ khÃ´ng."""
        logger.debug(f"ğŸ•µï¸ Checking validity of result for task '{result.task_id}' from miner '{result.miner_uid[:10]}...'")
        # Kiá»ƒm tra cáº¥u trÃºc result_data má»›i
        if not isinstance(result.result_data, dict) or "output_description" not in result.result_data:
            logger.warning(f"âš ï¸ Invalid result format for task '{result.task_id}' from miner '{result.miner_uid[:10]}...'. Missing 'output_description' in result_data.")
            return False
        logger.debug(f"âœ… Result for task '{result.task_id}' seems valid structure-wise.")
        return True

    def _generate_task_assignment(self, miner: 'MinerInfo') -> Optional['TaskAssignment']:
        """Táº¡o nhiá»‡m vá»¥ cá»¥ thá»ƒ cho miner (vÃ­ dá»¥: táº¡o prompt sinh áº£nh)."""
        # Táº¡o má»™t task_id duy nháº¥t
        task_id = self._generate_unique_task_id(miner.uid)
        logger.info(f"ğŸ“ Generating task '{task_id}' for miner '{miner.uid[:10]}...'")

        # Táº¡o task_data cá»¥ thá»ƒ cho Subnet 1 (vÃ­ dá»¥: prompt)
        try:
            prompt = self._generate_random_prompt()
            # Create task_data dict with 'description' key for the prompt
            # to match what the miner expects inside task_data
            task_data = {"description": prompt}
            logger.info(f"   Generated prompt: [italic green]'{prompt}'[/] for task '{task_id}'")

            assignment = TaskAssignment(
                task_id=task_id,
                miner_uid=miner.uid,
                task_data=task_data, # Assign the dict with 'description' key
                # Ensure TaskModel used by validator logic populates correctly
            )
            return assignment
        except Exception as e:
            logger.exception(f"ğŸ’¥ Error generating task data for miner '{miner.uid[:10]}...': {e}")
            return None

    def _generate_unique_task_id(self, miner_uid: str) -> str:
        """Generate unique task ID for a miner."""
        return f"task_{miner_uid[:8]}_{int(time.time())}_{uuid.uuid4().hex[:8]}"

    # --- CÃ¡c hÃ m helper tÃ¹y chá»n cho Subnet 1 --- 

    def _generate_random_prompt(self) -> str:
        """Táº¡o prompt ngáº«u nhiÃªn cho nhiá»‡m vá»¥ sinh áº£nh."""
        prompts = [
            "A photorealistic image of a cat wearing a wizard hat",
            "A watercolor painting of a futuristic city skyline at sunset",
            "A cute robot reading a book in a cozy library, digital art",
            "Impressionist painting of a sunflower field under a stormy sky",
            "A steaming cup of coffee on a wooden table, macro shot",
            "Pencil sketch of an ancient dragon sleeping on a pile of gold",
        ]
        return random.choice(prompts)

    # def _load_model(self):
    #     """Táº£i model sinh áº£nh (vÃ­ dá»¥: Stable Diffusion)."""
    #     logger.info("Loading image generation model...")
    #     # ... logic táº£i model ...
    #     logger.info("Image generation model loaded.")
    #     # return model

    # def _load_clip_scorer(self):
    #     """Táº£i model cháº¥m Ä‘iá»ƒm CLIP."""
    #     logger.info("Loading CLIP scorer...")
    #     # ... logic táº£i clip ...
    #     logger.info("CLIP scorer loaded.")
    #     # return scorer

    # def _decode_image(self, base64_string):
    #     """Giáº£i mÃ£ áº£nh tá»« chuá»—i base64."""
    #     # ... logic giáº£i mÃ£ ...
    #     # return image_object

    # === Main run method for backward compatibility ===
    async def run(self):
        """
        Main run method for backward compatibility.
        
        This method provides backward compatibility with existing validator scripts
        that expect a run() method on the validator instance.
        """
        logger.info(f"ğŸš€ Starting Subnet1Validator run method for UID: {self.info.uid}")
        
        try:
            # Use the new modular ValidatorNode interface with correct port
            await self.start(api_port=self.api_port)
            logger.info(f"âœ… Subnet1Validator started successfully")
            
            # Run until interrupted
            try:
                while True:
                    await asyncio.sleep(1)
            except KeyboardInterrupt:
                logger.info(f"ğŸ‘‹ Subnet1Validator stopped by user")
                
        except Exception as e:
            logger.error(f"âŒ Subnet1Validator run error: {e}")
            raise
        finally:
            await self.shutdown()
            logger.info(f"ğŸ Subnet1Validator run method finished")