import logging
import random
import base64
import time
import datetime
from typing import Any, Dict, List, Optional
from collections import defaultdict
import os
import binascii
import uuid
import sys
import asyncio

# Import t·ª´ SDK Moderntensor (ƒë√£ c√†i ƒë·∫∑t)
try:
    from moderntensor_aptos.mt_core.consensus.node import ValidatorNode
    from moderntensor_aptos.mt_core.core.datatypes import (
        TaskAssignment,
        MinerResult,
        ValidatorScore,
        ValidatorInfo,
        MinerInfo,
    )

    # Import successful, use real classes
    USING_MOCK_CLASSES = False
    logging.info("‚úÖ Successfully imported ValidatorNode and core datatypes from SDK")
except ImportError as e:
    logging.error(
        f"Could not import ValidatorNode or core datatypes from the SDK: {e}. "
        "Ensure the 'moderntensor' SDK is installed."
    )

    # L·ªõp gi·∫£ ƒë·ªÉ tr√°nh l·ªói n·∫øu import th·∫•t b·∫°i
    class ValidatorNode:
        def __init__(self, *args, **kwargs):
            self.validator_scores = {}
            self.results_received = defaultdict(list)
            self.tasks_sent = {}
            self.info = type("obj", (object,), {"uid": "fake_validator_uid"})()

        def _create_task_data(self, miner_uid: str) -> Any:
            return None

        # X√≥a score_miner_results gi·∫£ l·∫≠p

    class TaskAssignment:
        def __init__(self, task_id, miner_uid, task_data):
            self.task_id = task_id
            self.miner_uid = miner_uid
            self.task_data = task_data

    class MinerResult:
        def __init__(self, task_id, miner_uid, result_data):
            self.task_id = task_id
            self.miner_uid = miner_uid
            self.result_data = result_data

    class ValidatorScore:
        pass

    class ValidatorInfo:
        def __init__(self, uid, **kwargs):
            self.uid = uid

    class MinerInfo:
        def __init__(self, uid, **kwargs):
            self.uid = uid

    USING_MOCK_CLASSES = True

# Import t·ª´ c√°c module trong subnet n√†y
try:
    from .scoring.clip_scorer import calculate_clip_score
except ImportError:
    logging.error("Could not import scoring functions from .scoring.clip_scorer.")

    def calculate_clip_score(*args, **kwargs) -> float:
        return 0.0


logger = logging.getLogger(__name__)

DEFAULT_PROMPTS = [
    "A photorealistic image of an astronaut riding a horse on the moon.",
    "A watercolor painting of a cozy bookstore cafe in autumn.",
    "A synthwave style cityscape at sunset.",
    "A macro shot of a bee collecting pollen from a sunflower.",
    "A fantasy landscape with floating islands and waterfalls.",
    "A cute dog wearing sunglasses and a party hat.",
    "Impressionist painting of a Parisian street scene.",
    "A steaming bowl of ramen noodles with detailed ingredients.",
    "Cyberpunk warrior standing in a neon-lit alley.",
    "A tranquil zen garden with raked sand and stones.",
]


class Subnet1Validator(ValidatorNode):
    """
    Validator cho Subnet 1 (Image Generation).
    K·∫ø th·ª´a ValidatorNode v√† tri·ªÉn khai logic t·∫°o task, ch·∫•m ƒëi·ªÉm ·∫£nh.
    """

    def __init__(self, *args, **kwargs):
        """Kh·ªüi t·∫°o ValidatorNode v√† c√°c thu·ªôc t√≠nh ri√™ng c·ªßa Subnet 1."""
        # Extract api_port if provided
        self.api_port = kwargs.pop("api_port", None)

        super().__init__(*args, **kwargs)

        # Set reference to self in core for subnet-specific scoring access
        self.core.validator_instance = self

        logger.info(
            f"‚ú® [bold]Subnet1Validator[/] initialized for UID: [cyan]{self.info.uid[:10]}...[/]"
        )
        # Th√™m c√°c kh·ªüi t·∫°o kh√°c n·∫øu c·∫ßn, v√≠ d·ª•:
        # self.image_generation_model = self._load_model()
        # self.clip_scorer = self._load_clip_scorer()

    # --- 1. Override ph∆∞∆°ng th·ª©c t·∫°o Task Data ---
    def _create_task_data(self, miner_uid: str) -> Any:
        """
        T·∫°o d·ªØ li·ªáu task (prompt) ƒë·ªÉ g·ª≠i cho miner.
        *** ƒê√£ c·∫≠p nh·∫≠t ƒë·ªÉ th√™m validator_endpoint ***

        Args:
            miner_uid (str): UID c·ªßa miner s·∫Ω nh·∫≠n task (c√≥ th·ªÉ d√πng ƒë·ªÉ t√πy bi·∫øn task).

        Returns:
            Any: D·ªØ li·ªáu task, trong tr∆∞·ªùng h·ª£p n√†y l√† dict ch·ª©a prompt v√† validator_endpoint.
                 C·∫•u tr√∫c n√†y c·∫ßn ƒë∆∞·ª£c miner hi·ªÉu.
        """
        selected_prompt = random.choice(DEFAULT_PROMPTS)
        logger.debug(
            f"Creating task for miner {miner_uid} with prompt: '{selected_prompt}'"
        )

        # L·∫•y API endpoint c·ªßa ch√≠nh validator n√†y t·ª´ self.info
        # C·∫ßn ƒë·∫£m b·∫£o self.info v√† self.info.api_endpoint ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o ƒë√∫ng
        origin_validator_endpoint = getattr(self.info, "api_endpoint", None)
        if not origin_validator_endpoint:
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p endpoint kh√¥ng c√≥ s·∫µn (quan tr·ªçng)
            logger.error(
                f"Validator {getattr(self.info, 'uid', 'UNKNOWN')} has no api_endpoint configured in self.info. Cannot create task properly."
            )
            # C√≥ th·ªÉ tr·∫£ v·ªÅ None ho·∫∑c raise l·ªói ƒë·ªÉ ngƒÉn g·ª≠i task kh√¥ng ƒë√∫ng
            return None  # Ho·∫∑c raise ValueError("Validator endpoint missing")

        # T·∫°o deadline v√≠ d·ª• (v√≠ d·ª•: 5 ph√∫t k·ªÉ t·ª´ b√¢y gi·ªù)
        now = datetime.datetime.now(datetime.timezone.utc)
        deadline_dt = now + datetime.timedelta(minutes=5)
        deadline_str = deadline_dt.isoformat()

        # ƒê·∫∑t priority m·∫∑c ƒë·ªãnh
        priority_level = random.randint(1, 5)

        # Tr·∫£ v·ªÅ dictionary ch·ª©a c√°c tr∆∞·ªùng c·∫ßn thi·∫øt CHO MINER HI·ªÇU
        # Miner s·∫Ω c·∫ßn ƒë·ªçc 'description' ƒë·ªÉ l·∫•y prompt
        # Miner s·∫Ω c·∫ßn ƒë·ªçc 'validator_endpoint' ƒë·ªÉ bi·∫øt g·ª≠i k·∫øt qu·∫£ v·ªÅ ƒë√¢u
        return {
            "description": selected_prompt,  # Prompt ch√≠nh l√† description c·ªßa task
            "deadline": deadline_str,
            "priority": priority_level,
            "validator_endpoint": origin_validator_endpoint,  # <<<--- TH√äM D√íNG N√ÄY
        }

    # --- Restore the correct override method for scoring ---
    def _score_individual_result(self, task_data: Any, result_data: Any) -> float:
        """
        (Override) Ch·∫•m ƒëi·ªÉm cho m·ªôt k·∫øt qu·∫£ c·ª• th·ªÉ t·ª´ miner cho Subnet 1.
        This method is called by the base ValidatorNode class during its scoring phase.

        Args:
            task_data: D·ªØ li·ªáu c·ªßa task ƒë√£ g·ª≠i (dict ch·ª©a 'description' l√† prompt).
            result_data: D·ªØ li·ªáu k·∫øt qu·∫£ miner tr·∫£ v·ªÅ (dict ch·ª©a 'output_description', etc.).

        Returns:
            ƒêi·ªÉm s·ªë float t·ª´ 0.0 ƒë·∫øn 1.0.
        """
        logger.debug(f"üíØ Scoring result via _score_individual_result...")
        score = 0.0  # Default score
        start_score_time = time.time()
        try:
            # 1. Extract prompt and base64 image
            if not isinstance(task_data, dict) or "description" not in task_data:
                logger.warning(
                    f"Scoring failed: Task data is not a dict or missing 'description'. Task data: {str(task_data)[:100]}..."
                )
                return 0.0
            original_prompt = task_data["description"]

            if not isinstance(result_data, dict):
                logger.warning(
                    f"Scoring failed: Received result_data is not a dictionary. Data: {str(result_data)[:100]}..."
                )
                return 0.0
            image_base64 = result_data.get("output_description")
            reported_error = result_data.get("error_details")
            processing_time_ms = result_data.get("processing_time_ms", 0)  # Optional

            # 2. Check for errors or missing image
            if reported_error:
                logger.warning(
                    f"Miner reported an error: '{reported_error}'. Assigning score 0."
                )
                return 0.0
            if not image_base64 or not isinstance(image_base64, str):
                logger.warning(
                    f"No valid image data (base64 string) found in result_data. Assigning score 0. Data: {str(result_data)[:100]}..."
                )
                return 0.0

            # 3. Decode image and Save it
            try:
                image_bytes = base64.b64decode(image_base64)

                # --- Start: Save Image Logic ---
                output_dir = "result_image"
                try:
                    os.makedirs(output_dir, exist_ok=True)
                    # Using placeholder name for now, ideally pass task_id here.
                    miner_uid = result_data.get("miner_uid", "unknown_miner")
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
                    # Need task_id for a truly unique name. Placeholder:
                    filename = f"{output_dir}/result_{miner_uid[:8]}_{timestamp}.png"
                    with open(filename, "wb") as f:
                        f.write(image_bytes)
                    logger.info(f"   Saved image result to: {filename}")
                except OSError as file_err:
                    logger.error(
                        f"   Error saving image file to {filename}: {file_err}"
                    )
                except Exception as e:
                    logger.exception(f"   Unexpected error saving image: {e}")
                # --- End: Save Image Logic ---

            except (binascii.Error, ValueError, TypeError) as decode_err:
                logger.error(
                    f"Scoring failed: Invalid base64 data received. Error: {decode_err}. Assigning score 0."
                )
                return 0.0  # Return 0 if decode fails

            # 4. Calculate CLIP Score
            score = calculate_clip_score(image_bytes, original_prompt)
            # Ensure score is within valid range
            score = max(0.0, min(1.0, score))

            logger.info(
                f"   üìä CLIP Score: {score:.4f} for prompt: '{original_prompt[:50]}...'"
            )

        except Exception as e:
            logger.exception(f"Scoring failed with exception: {e}")
            score = 0.0

        scoring_duration = time.time() - start_score_time
        logger.debug(
            f"üíØ Scoring completed in {scoring_duration:.3f}s, score: {score:.4f}"
        )
        return score

    # --- 2. Override ph∆∞∆°ng th·ª©c x·ª≠ l√Ω k·∫øt qu·∫£ ---
    def _should_process_result(self, result: MinerResult) -> bool:
        """
        (Override) X√°c ƒë·ªãnh c√≥ n√™n x·ª≠ l√Ω k·∫øt qu·∫£ n√†y kh√¥ng.
        """
        # Implement any custom logic here
        # For now, process all results
        return True

    # --- 3. Override ph∆∞∆°ng th·ª©c t·∫°o Task Assignment ---
    def _generate_task_assignment(
        self, miner: "MinerInfo"
    ) -> Optional["TaskAssignment"]:
        """
        (Override) T·∫°o task assignment cho miner.
        """
        task_id = self._generate_unique_task_id(miner.uid)
        task_data = self._create_task_data(miner.uid)

        if task_data is None:
            logger.warning(f"Could not create task data for miner {miner.uid}")
            return None

        return TaskAssignment(task_id=task_id, miner_uid=miner.uid, task_data=task_data)

    # --- 4. Helper methods ---
    def _generate_unique_task_id(self, miner_uid: str) -> str:
        """Generate a unique task ID."""
        timestamp = int(time.time() * 1000)
        return f"task_{miner_uid[:8]}_{timestamp}"

    def _generate_random_prompt(self) -> str:
        """Generate a random prompt for testing."""
        return random.choice(DEFAULT_PROMPTS)

    # --- 5. Override run method n·∫øu c·∫ßn ---
    async def run(self):
        """
        Main run loop cho validator.
        """
        logger.info(f"üöÄ Starting Subnet1Validator for UID: {self.info.uid}")
        try:
            # Call parent run method
            await super().run()
        except Exception as e:
            logger.exception(f"Error in Subnet1Validator.run(): {e}")
            raise

    # --- 6. Additional methods for subnet-specific functionality ---
    def get_validator_stats(self) -> Dict[str, Any]:
        """
        L·∫•y th·ªëng k√™ c·ªßa validator.
        """
        return {
            "uid": self.info.uid,
            "tasks_sent": len(self.tasks_sent),
            "results_received": sum(
                len(results) for results in self.results_received.values()
            ),
            "validator_scores": len(self.validator_scores),
            "api_port": self.api_port,
            "using_mock_classes": USING_MOCK_CLASSES,
        }
